{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d5db4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-19T19:46:51.133225Z",
     "iopub.status.busy": "2022-04-19T19:46:51.131398Z",
     "iopub.status.idle": "2022-04-19T19:46:53.284806Z",
     "shell.execute_reply": "2022-04-19T19:46:53.283981Z",
     "shell.execute_reply.started": "2022-04-19T19:46:13.006462Z"
    },
    "papermill": {
     "duration": 2.171529,
     "end_time": "2022-04-19T19:46:53.284978",
     "exception": false,
     "start_time": "2022-04-19T19:46:51.113449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243f40c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T19:46:53.314932Z",
     "iopub.status.busy": "2022-04-19T19:46:53.314288Z",
     "iopub.status.idle": "2022-04-19T19:46:53.328578Z",
     "shell.execute_reply": "2022-04-19T19:46:53.329152Z",
     "shell.execute_reply.started": "2022-04-19T19:46:15.475427Z"
    },
    "papermill": {
     "duration": 0.029965,
     "end_time": "2022-04-19T19:46:53.329330",
     "exception": false,
     "start_time": "2022-04-19T19:46:53.299365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pearsonr(preds: np.array, dset: lgb.Dataset):\n",
    "    \"\"\"\n",
    "    Helper function to compute Pearson correlation \n",
    "    on validation dataset for LightGBM as tracking metric.\n",
    "    Args:\n",
    "        preds: 1d-array with the model predictions\n",
    "        dset: LightGBM dataset with the labels\n",
    "    Returs:\n",
    "        Tuple with the corresponding output\n",
    "    \"\"\"\n",
    "    labels = dset.get_label() # 获取lgb.Dataset的label\n",
    "    return 'pearsonr', stats.pearsonr(preds, labels)[0], True \n",
    "\n",
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \"\"\"\n",
    "    Utility function to reduce the memory usage of pandas dataframes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.Dataframe\n",
    "    verbose: Boolean\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # -128 to 127\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                \n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5786652",
   "metadata": {
    "papermill": {
     "duration": 0.012522,
     "end_time": "2022-04-19T19:46:55.198885",
     "exception": false,
     "start_time": "2022-04-19T19:46:55.186363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef490043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T19:46:55.231958Z",
     "iopub.status.busy": "2022-04-19T19:46:55.231275Z",
     "iopub.status.idle": "2022-04-19T19:50:38.434654Z",
     "shell.execute_reply": "2022-04-19T19:50:38.433907Z"
    },
    "papermill": {
     "duration": 223.223454,
     "end_time": "2022-04-19T19:50:38.434958",
     "exception": false,
     "start_time": "2022-04-19T19:46:55.211504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feat_names = [f\"f_{i}\" for i in range(300)] # 300个特征名称\n",
    "col_names = feat_names + [\"target\"] # 特征 + label\n",
    "data_dir = \"../input/ubiquant-market-prediction\" # 数据目录\n",
    "\n",
    "# 读取训练数据\n",
    "if os.path.exists(f\"{data_dir}/train.pkl\"):\n",
    "    train_data = pd.read_pickle(f\"{data_dir}/train.pkl\") \n",
    "else:\n",
    "    train_data = pd.read_csv(f\"{data_dir}/train.csv\", usecols=col_names)\n",
    "    train_data = reduce_mem_usage(train_data, verbose=True) # 减少内存占用\n",
    "    train_data.to_pickle(f\"{data_dir}/train.pkl\") # 保存训练数据\n",
    "    gc.collect()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "from sklearn.model_selection import TimeSeriesSplit # 导入时间序列交叉验证\n",
    "tscv = TimeSeriesSplit(5) # 5折时间序列交叉验证\n",
    "for fold, (trn_ind, val_ind) in enumerate(tscv.split(train_data)):\n",
    "    # print(f\"train length: {len(trn_ind)}, valid length: {len(val_ind)}\")\n",
    "    train_df = train_data.loc[trn_ind, :] # 训练集\n",
    "    valid_df = train_data.loc[val_ind, :] # 验证集\n",
    "    \n",
    "print(f\"train_df.shape:{train_df.shape}; valid_df.shape:{valid_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5ee9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T19:50:38.475660Z",
     "iopub.status.busy": "2022-04-19T19:50:38.474918Z",
     "iopub.status.idle": "2022-04-19T19:50:39.621774Z",
     "shell.execute_reply": "2022-04-19T19:50:39.621161Z"
    },
    "papermill": {
     "duration": 1.173548,
     "end_time": "2022-04-19T19:50:39.621927",
     "exception": false,
     "start_time": "2022-04-19T19:50:38.448379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dset = lgb.Dataset(\n",
    "    data=train_df[feat_names], # 训练集特征\n",
    "    label=train_df[\"target\"].values, # 训练集label\n",
    "    free_raw_data=False, # 关闭原始数据的内存占用\n",
    ")\n",
    "\n",
    "valid_dset = lgb.Dataset(\n",
    "    data=valid_df[feat_names], # 验证集特征\n",
    "    label=valid_df[\"target\"].values, # 验证集label\n",
    "    free_raw_data=False, # 关闭原始数据的内存占用\n",
    ")\n",
    "\n",
    "del train_data\n",
    "del train_df\n",
    "del valid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna # 导入optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model_params = {\n",
    "        'boosting': 'dart', # dart 提升算法 DART: Dropouts meet Multiple Additive Regression Trees https://arxiv.org/abs/1505.01866 \n",
    "        'linear_tree': True, # 线性树\n",
    "        'objective': 'mse', # 均方误差 损失函数\n",
    "        'metric': 'rmse', # 均方根误差 评估函数\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.005, 0.1), # 学习率\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 10, 64), # 最大叶子数量\n",
    "        'max_bin': trial.suggest_int(\"max_bin\", 200, 600), # 最大分箱数\n",
    "        'force_col_wise': True, # 强制建立列直方图，可以减少内存占用\n",
    "        'bagging_freq': 1, # 每 k 次迭代执行bagging\n",
    "        'seed': 42, # 随机种子\n",
    "        'verbosity': -1, # 不输出\n",
    "        'first_metric_only': False, # 计算多个评估指标\n",
    "        'bin_construct_sample_cnt': 100000000, # 分箱构造样本数量\n",
    "        'feature_pre_filter': False, # 特征预过滤\n",
    "        'bagging_fraction': 1.0, # 不进行重采样的情况下随机选择部分数据\n",
    "        'drop_rate': 0.05, # 丢弃率（树） \n",
    "        'feature_fraction': trial.suggest_discrete_uniform(\"feature_fraction\", 0.05, 0.5, 0.1), # 特征采样比例\n",
    "        'lambda_l1': 3.2608153782775893, # L1正则化 \n",
    "        'lambda_l2': 24.65715474841406, # L2正则化\n",
    "        'linear_lambda': 15.831719022196562, # 线性回归正则化\n",
    "        'max_drop': 5, # 在一次提升迭代中被丢弃的树的最大数量\n",
    "        'min_data_in_leaf': 2200, # 叶子节点最少样本数\n",
    "        'num_iterations': 1900, # 迭代次数\n",
    "        'path_smooth': 4.714076496843463, # 树节点的平滑度, 有助于防止对样本少的树叶进行过度拟合 ####\n",
    "        'skip_drop': 0.65 # 跳过丢弃的概率\n",
    "    }\n",
    "\n",
    "    _model_params = dict(model_params)\n",
    "    _model_params[\"seed\"] = 42 # 随机种子\n",
    "    \n",
    "    log_callback = lgb.log_evaluation(period=20) # 训练日志频率\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params=_model_params, # 参数\n",
    "        train_set=train_dset, # 训练集\n",
    "        valid_sets=[train_dset, valid_dset], # 验证集\n",
    "        feval=pearsonr, # 评估函数\n",
    "        callbacks=[log_callback,], # 训练日志\n",
    "    )\n",
    "    \n",
    "    lgb.plot_importance(model, figsize=(8,15), importance_type=\"split\", max_num_features=30) # split 特征重要度\n",
    "    lgb.plot_importance(model, figsize=(8,15), importance_type=\"gain\", max_num_features=30) # gain 特征重要度\n",
    "    plt.show()\n",
    "\n",
    "    return model.best_score[\"valid_1\"][\"pearsonr\"] # best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9aba62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T19:50:39.657554Z",
     "iopub.status.busy": "2022-04-19T19:50:39.652739Z",
     "iopub.status.idle": "2022-04-19T19:50:39.660086Z",
     "shell.execute_reply": "2022-04-19T19:50:39.659437Z"
    },
    "papermill": {
     "duration": 0.024375,
     "end_time": "2022-04-19T19:50:39.660229",
     "exception": false,
     "start_time": "2022-04-19T19:50:39.635854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize') # 创建study，最大化score\n",
    "study.optimize(objective, n_trials=100) # 进行100次试验"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2221.215846,
   "end_time": "2022-04-19T20:23:42.208448",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-19T19:46:40.992602",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b79a61544c9a744d09395b396d14bdc3ab2980641b64ddb1c7bc6d7b892900a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
